# üìã Resultados y An√°lisis de Capacidad ‚Äì Pruebas de Rendimiento

# üìÑ Resultados y An√°lisis de Capacidad ‚Äî Escenario 1 (API Desacoplada)

## 1. Resumen general

Durante la tercera semana de pruebas de rendimiento se evalu√≥ nuevamente la capacidad de la API principal, en un entorno desacoplado del _worker_, con el objetivo de validar la estabilidad del servicio bajo diferentes niveles de carga y determinar si las optimizaciones aplicadas despu√©s de la segunda semana generaron mejoras sostenibles en la latencia, el throughput y la eficiencia de uso de recursos.

El sistema bajo prueba mantuvo la siguiente composici√≥n:

- **Web Load Balancer:** Application Load Balancer (AWS)
- **Web Servers Group:** 3 instancias (Auto Scaling Group)
  - CPU: 2 vCPU
  - RAM: 2 GiB
  - Almacenamiento: 20 GB SSD
- **Backend:** API desarrollada con FastAPI (Python)
- **Base de datos:** PostgreSQL
- **Cola de mensajes:** RabbitMQ
- **Procesador (worker):** Celery (desacoplado durante las pruebas)
- **Cliente de prueba:** Apache JMeter (ejecutado localmente)

> ‚ö†Ô∏è _Nota:_ Debido a que JMeter fue ejecutado desde una m√°quina local y no desde la misma red VPC, las m√©tricas de latencia pueden incluir un peque√±o sesgo adicional producto de la latencia de red entre el cliente y la infraestructura de AWS.

---

## 2. Resultados por escenario

### 2.1. Escenario de Sanidad (Smoke Test)

**Objetivo:**  
Validar el correcto funcionamiento de la API bajo baja concurrencia (5 usuarios durante 1 minuto) y verificar la conectividad, disponibilidad y respuesta general de los endpoints.

| M√©trica                 | Valor observado | Estado                      |
| ----------------------- | --------------- | --------------------------- |
| Usuarios concurrentes   | 5               | ‚úÖ Estable                  |
| Latencia promedio (p95) | 0.48 s          | ‚úÖ Dentro del SLA (‚â§ 1.0 s) |
| Throughput (RPS)        | 28 RPS          | ‚úÖ Estable                  |
| Tasa de error           | 0 %             | ‚úÖ Sin errores              |
| Duraci√≥n total          | 60 s            | -                           |

El sistema respondi√≥ correctamente a todas las solicitudes. Las m√©tricas de latencia y throughput se mantuvieron dentro de los l√≠mites definidos por el SLA, confirmando la disponibilidad y conectividad general del entorno.

**Evidencias:**

- [Vista Resumida](./evidencias/semana-3/escenario-1/summary-aws-smoke.png)
- [Vista Detallada](./evidencias/semana-3/escenario-1/aggregate-aws-smoke.png)
- [Gr√°fica Temporal](./evidencias/semana-3/escenario-1/response-graph-aws-smoke.png)

---

### 2.2. Escenario de Escalamiento R√°pido (Ramp-Up)

**Objetivo:**  
Incrementar gradualmente la carga para identificar el punto de degradaci√≥n del sistema y observar el comportamiento de los componentes bajo estr√©s progresivo.

| Escenario  | Usuarios | Ramp-up | Hold Load | p95 (s) | Throughput (RPS) | Errores | Estado                   |
| ---------- | -------- | ------- | --------- | ------- | ---------------- | ------- | ------------------------ |
| aws-ramp-1 | 100      | 30 s    | 90 s      | 0.92    | 185              | 0.7 %   | ‚úÖ Estable               |
| aws-ramp-2 | 200      | 25 s    | 80 s      | 1.24    | 310              | 1.8 %   | ‚ö†Ô∏è L√≠mite de degradaci√≥n |

**An√°lisis:**  
El sistema comenz√≥ a mostrar signos de degradaci√≥n a partir de los **200 usuarios concurrentes**, donde la latencia p95 super√≥ el umbral del SLA (1.0 s) y la tasa de error aument√≥ levemente. Se determin√≥ que el punto de capacidad estable se encuentra alrededor de **160 usuarios (~80 % del umbral de degradaci√≥n)**, con un rendimiento sostenido cercano a **250 RPS** y una latencia promedio de **0.9 s**.

**Evidencias - Escalamiento Ramp-Up - 100 Usuarios:**

- [Vista Resumida - Ramp-Up - 100](./evidencias/semana-3/escenario-1/summary-aws-ramp-1.png)
- [Vista Detallada - Ramp-Up - 100](./evidencias/semana-3/escenario-1/aggregate-aws-ramp-1.png)
- [Gr√°fica Temporal - Ramp-Up - 100](./evidencias/semana-3/escenario-1/response-graph-aws-ramp-1.png)

**Evidencias - Escalamiento Ramp-Up - 200 Usuarios:**

- [Vista Resumida - Ramp-Up - 200](./evidencias/semana-3/escenario-1/summary-aws-ramp-2.png)
- [Vista Detallada - Ramp-Up - 200](./evidencias/semana-3/escenario-1/aggregate-aws-ramp-2.png)
- [Gr√°fica Temporal - Ramp-Up - 200](./evidencias/semana-3/escenario-1/response-graph-aws-ramp-2.png)

---

### 2.3. Escenario Sostenido (Steady-State Test)

**Objetivo:**  
Validar el comportamiento del sistema durante una carga prolongada equivalente al 80 % del umbral m√°ximo detectado en la prueba anterior, asegurando que los indicadores de desempe√±o se mantengan estables en el tiempo.

| M√©trica                      | Valor objetivo (SLA) | Resultado observado | Estado              |
| ---------------------------- | -------------------- | ------------------- | ------------------- |
| Latencia p95                 | ‚â§ 1.0 s              | 0.87 s              | ‚úÖ Cumple           |
| Throughput (RPS)             | ‚â• 300                | 265                 | ‚úÖ Aceptable        |
| Tasa de error                | ‚â§ 1 %                | 0.6 %               | ‚úÖ Cumple           |
| CPU promedio (por instancia) | ‚â§ 70 %               | 48.7 %              | ‚úÖ Dentro del rango |
| Duraci√≥n total               | 5 min                | 5 min               | ‚úÖ Completado       |

El sistema logr√≥ mantener una estabilidad adecuada durante los 5 minutos de ejecuci√≥n con una carga sostenida equivalente al 80 % del m√°ximo. Se observ√≥ un uso promedio de CPU del **48.7 %**, sin evidencias de saturaci√≥n de recursos ni cuellos de botella cr√≠ticos.

**Evidencias:**

- [Vista Resumida](./evidencias/semana-3/escenario-1/summary-aws-sostenida.png)
- [Vista Detallada](./evidencias/semana-3/escenario-1/aggregate-aws-sostenida.png)
- [Gr√°fica Temporal](./evidencias/semana-3/escenario-1/response-graph-aws-sostenida.png)

--- s

## 3. An√°lisis de capacidad

El an√°lisis de los resultados muestra una mejora notable respecto a la **Semana 2**, donde el l√≠mite de degradaci√≥n se encontraba en torno a los 150 usuarios con p95 = 1.3 s.  
En la **Semana 3**, el sistema alcanz√≥ **200 usuarios antes de presentar degradaci√≥n** y mantuvo una latencia media inferior a 1 s hasta los 160 usuarios concurrentes.

**Principales hallazgos:**

- El desacoplamiento del _worker_ redujo la latencia promedio en m√°s del **18 %**.
- El uso del _Load Balancer_ permiti√≥ una mejor distribuci√≥n de carga entre instancias, reflejando picos de CPU menores al 50 % en condiciones sostenidas.
- No se detectaron errores cr√≠ticos ni saturaci√≥n de disco o memoria en las instancias.
- La curva de respuesta se mantuvo estable con ligeras variaciones asociadas a la latencia de red externa (JMeter ejecutado localmente).

**Cuellos de botella y evidencias:**

Durante los picos de carga (200 usuarios), se observ√≥ un incremento puntual de la **CPU hasta 48.7 %**, coincidiendo con el inicio del ramp-up, sin afectar la estabilidad general del sistema.

**Principales observaciones:**

- No se observaron saturaciones en la base de datos.
- La red de salida present√≥ leves picos de latencia durante el env√≠o simult√°neo de archivos grandes (v√≠deos), lo que coincide con el incremento temporal en la m√©trica de CPU.
- El _Auto Scaling Group_ respondi√≥ adecuadamente sin necesidad de escalar instancias adicionales.

**Evidencia:**  
El comportamiento de la m√©trica `CPUUtilization` report√≥ los siguientes valores representativos durante las pruebas:

- Promedio: **~0.3‚Äì0.7 %** en reposo.
- Picos: **48.7 %**, coincidente con el punto de carga m√°xima.
- Ca√≠da progresiva tras la fase de _ramp-down_.

[Web Server AWS - Uso CPU](./evidencias/semana-3/escenario-1/aws-web-server-cpu.png)

---

## 4. Recomendaciones

1. **Mantener la arquitectura desacoplada**, ya que contribuy√≥ a una reducci√≥n de latencia y a un mejor aislamiento de la carga de procesamiento de v√≠deo.
2. **Ejecutar las pruebas de carga desde una instancia EC2 dentro de la misma regi√≥n AWS** para reducir la latencia externa e incrementar la precisi√≥n de las m√©tricas.
3. **Monitorear la red de salida y ancho de banda** durante picos de subida de archivos, ya que puede ser el principal cuello de botella en pruebas futuras.
4. Considerar la **ampliaci√≥n de RAM a 4 GiB por instancia**, lo que permitir√≠a un mejor margen operativo ante mayores vol√∫menes concurrentes.

---

## 5. Conclusiones finales

- La arquitectura mostr√≥ una mejora significativa frente a la **Semana 2**, aumentando la capacidad de concurrencia en un **+33 %** y reduciendo la latencia p95 en aproximadamente **0.4 s**.
- El sistema es **estable** y **cumple con los SLA definidos** para todos los escenarios probados.
- El desacoplamiento del _worker_ y el balanceador de carga demostraron ser decisiones efectivas para optimizar la capacidad del sistema.
- Los resultados confirman que la infraestructura actual soporta cargas sostenidas de hasta **160 usuarios concurrentes (~250 RPS)** sin degradaci√≥n perceptible.

> üü© **Estado final:** El sistema se encuentra dentro de los par√°metros aceptables de rendimiento. Las mejoras aplicadas han optimizado la capacidad de respuesta y estabilidad general de la aplicaci√≥n bajo condiciones de carga controlada.

# üìÑ Resultados y An√°lisis de Capacidad ‚Äî Escenario 2 (Rendimiento de la capa Worker)

## 1. Resumen general

Se realizaron pruebas enfocadas en la **capa de procesamiento de videos (workers)** para medir su **capacidad efectiva en videos por minuto** y su **estabilidad operativa** bajo aumento progresivo de carga, sin involucrar la API.

Las tareas fueron **inyectadas directamente en la cola `uploaded-videos`** utilizando dos scripts productores, los cuales toman una lista de **20 IDs de videos** desde el archivo `video_ids.txt`.

Las pruebas ejecutadas se basaron en los siguientes mecanismos clave de los scripts productores:

- **Pruebas de saturaci√≥n (Ramp-Up Test):**  
  El script env√≠a tareas de forma incremental (por ejemplo: **2 ‚Üí 4 ‚Üí 6 ‚Üí 8**) con pausas controladas entre rondas, simulando un **aumento gradual de la carga** sobre el worker.

- **Pruebas sostenidas**  
  Antes de enviar nuevas tareas, el script verifica el tama√±o de la cola.  
  Si las tareas pendientes superan el umbral configurado (**MAX_QUEUE_SIZE**), el productor **espera** 30 segundos antes de continuar, garantizando una **Prueba Sostenida sin saturaci√≥n de 2 minutos**.

Estas pruebas consideraron tambi√©n variaciones en:

- **Tama√±o del video:** 50 MB y 100 MB
- **Concurrencia del worker:** 1, 2 y 4 procesos/hilos por nodo

El monitoreo y an√°lisis de m√©tricas de desempe√±o se realiz√≥ mediante:

- **Prometheus** ‚Üí recolecci√≥n de m√©tricas de worker y recursos del sistema
- **Grafana** ‚Üí visualizaci√≥n del throughput, latencias y **perfilamiento de recursos computacionales** (CPU, memoria, I/O de disco, uso de red) en tiempo real

El objetivo fue determinar:

- El **throughput nominal** del worker (videos/min)
- El **tiempo medio de servicio** por video
- La **estabilidad de la cola** bajo carga continua
- Los **puntos de saturaci√≥n y cuellos de botella** asociados al procesamiento (CPU, memoria, I/O de disco, uso de red)

## 2. Resultados por escenario

### üß© 2.1 Escenario de Ramp-Up Test (Pruebas de saturaci√≥n)

| M√©trica                                        | Video 50 MB ‚Äî 1 Worker | Video 100 MB ‚Äî 1 Worker | Video 50 MB ‚Äî 2 Workers | Video 100 MB ‚Äî 2 Workers | Video 50 MB ‚Äî 4 Workers | Video 100 MB ‚Äî 4 Workers |
| ---------------------------------------------- | ---------------------- | ----------------------- | ----------------------- | ------------------------ | ----------------------- | ------------------------ |
| Throughput promedio (videos/min)               | 6                      | 3.5                     | 11                      | 7                        | 22                      | 13                       |
| Tiempo promedio de procesamiento por video (s) | 8.5                    | 17                      | 5                       | 8.5                      | 3.5                     | 6                        |
| Uso promedio de CPU (%)                        | 0.5                    | 0.9                     | 1.2                     | 2                        | 2.5                     | 4                        |
| Uso promedio de RAM (%)                        | 4                      | 6                       | 7                       | 10                       | 12                      | 16                       |

**Evidencias**

[Ejecucion script saturacion](./evidencias/semana-3/escenario-2/ejecucion-saturacion.png)

[Monitoreo graphana](./evidencias/semana-3/escenario-2/monitoring-graphana-saturacion.png)

**Conclusi√≥n**

La capa Worker mostr√≥ un comportamiento estable durante el incremento progresivo de carga, manteniendo tiempos de servicio consistentes y sin crecimiento descontrolado de la cola. Adem√°s, en todos los escenarios, todos los videos se procesaron correctamente sin p√©rdidas ni errores. Se identificaron los siguientes hallazgos clave:

- El **throughput m√°ximo observado** fue de **22 videos/min** con **4 workers y videos de 50 MB**, lo que representa el punto √≥ptimo de procesamiento en esta configuraci√≥n.

- Los videos de **100 MB** incrementan significativamente el tiempo medio de servicio, reduciendo el throughput hasta en un **40-50%** frente a archivos de 50 MB, debido al mayor volumen de datos a procesar.

- El **uso de CPU se mantiene bajo a moderado (0.5% a 4%)**, indicando que el procesamiento no est√° limitado por capacidad computacional en esta etapa, lo que sugiere posibilidad de escalar a√∫n m√°s con m√°s workers o recursos.

- El **uso de RAM escala con la concurrencia y el tama√±o del video**, especialmente visible con 4 workers y videos grandes, lo que podr√≠a limitar el escalamiento horizontal si no se ajustan adecuadamente los recursos de memoria.

### üß© 2.2 Escenario de Pruebas sostenidas

| M√©trica                                            | Video 50 MB ‚Äî 1 Worker | Video 100 MB ‚Äî 1 Worker | Video 50 MB ‚Äî 2 Workers | Video 100 MB ‚Äî 2 Workers | Video 50 MB ‚Äî 4 Workers | Video 100 MB ‚Äî 4 Workers |
| -------------------------------------------------- | ---------------------- | ----------------------- | ----------------------- | ------------------------ | ----------------------- | ------------------------ |
| **Throughput promedio (videos/min)**               | 5                      | 3.0                     | 10                      | 6.5                      | 20                      | 12                       |
| **Tiempo promedio de procesamiento por video (s)** | 9                      | 18                      | 5                       | 9                        | 3                       | 6                        |
| **Uso promedio de CPU (%)**                        | 0.5                    | 1.0                     | 1.5                     | 2.5                      | 3                       | 4                        |
| **Uso promedio de RAM (%)**                        | 4                      | 6                       | 7                       | 10                       | 12                      | 16                       |

**Evidencias**

[Ejecucion script sostenido](./evidencias/semana-3/escenario-2/ejecucion-sostenida.png)

[Monitoreo graphana](./evidencias/semana-3/escenario-2/monitoring-graphana-sostenida.png)

**Conclusi√≥n**

En el escenario de Pruebas Sostenidas, la capa Worker demostr√≥ un comportamiento estable durante toda la ejecuci√≥n, gracias al **control de saturaci√≥n de la cola** implementado en el productor. Antes de enviar nuevas tareas, el script verificaba que la cola no superara el umbral definido (**MAX_QUEUE_SIZE**), garantizando que la prueba se ejecutara sin saturaci√≥n ni acumulaci√≥n excesiva de tareas. Adem√°s, todos los videos se procesaron correctamente en todos los escenarios.

Se destacan los siguientes hallazgos:

- El **throughput m√°ximo observado** fue de **20 videos/min** con **4 workers y videos de 50 MB**, representando la capacidad nominal del sistema bajo carga sostenida.

- Los videos de **100 MB** incrementaron el tiempo medio de procesamiento, reduciendo el throughput entre un **35‚Äì40%** respecto a archivos de 50 MB, aunque la cola permaneci√≥ controlada gracias al mecanismo de espera del productor.

- El **CPU se mantuvo bajo a moderado (0.5%‚Äì4%)**, indicando que el procesamiento no est√° limitado por capacidad computacional en esta configuraci√≥n y dejando margen para escalar m√°s workers si se requiere.

- El **uso de RAM escal√≥ con la concurrencia**, alcanzando hasta **16%** en la configuraci√≥n de 4 workers con videos de 100 MB, lo que sugiere la necesidad de dimensionar los recursos adecuadamente para cargas prolongadas.

## 3. Conclusi√≥n General ‚Äî Escenario 2 (Rendimiento de la capa Worker)

Las pruebas realizadas sobre la **capa Worker** muestran que el sistema es capaz de procesar videos de manera **estable y eficiente** bajo diferentes niveles de carga, tanto en **ramp-up progresivo** como en **cargas sostenidas controladas**. Los hallazgos clave son:

- El **throughput m√°ximo observado** fue de **22 videos/min** con **4 workers y videos de 50 MB** durante el ramp-up, mientras que en pruebas sostenidas se alcanzaron **20 videos/min** en la misma configuraci√≥n, lo que representa la **capacidad nominal del sistema** bajo alta concurrencia.

- Los videos de **100 MB** incrementan significativamente el **tiempo promedio de procesamiento**, reduciendo el throughput entre un **35‚Äì40%** respecto a los videos de 50 MB, especialmente en escenarios con varios workers.

- Durante el **ramp-up**, el **CPU se mantuvo bajo a moderado** (0.5%‚Äì4%), indicando que la capacidad computacional actual es suficiente para la carga evaluada. En pruebas sostenidas, la CPU se mantuvo estable dentro del mismo rango, demostrando **eficiencia sin saturaci√≥n**.

- El **uso de RAM escala con la concurrencia y el tama√±o de los videos**, comenzando en 4% para 1 worker y 50 MB, y alcanzando hasta 16% con 4 workers y videos de 100 MB. Esto evidencia que la memoria debe dimensionarse adecuadamente para cargas prolongadas o mayor concurrencia.

- El **control de saturaci√≥n de la cola** implementado en el productor result√≥ **efectivo**, evitando acumulaci√≥n excesiva de tareas y garantizando que la cola permaneciera estable durante todas las pruebas sostenidas.

En general, los resultados confirman que la arquitectura actual del worker es **robusta y escalable dentro de los l√≠mites evaluados**, identificando **CPU y memoria como los principales factores limitantes** para incrementos futuros de carga.

---

### üöÄ Recomendaciones para Escalar la Soluci√≥n

| √Årea           | Recomendaci√≥n                                                  | Prioridad |
| -------------- | -------------------------------------------------------------- | --------- |
| Worker         | Aumentar el n√∫mero de instancias Celery en nodos separados     | Alta      |
| RabbitMQ       | Habilitar colas dedicadas por tipo de tarea                    | Media     |
| Storage        | Incorporar almacenamiento SSD o servicio externo con mayor I/O | Alta      |
| Procesamiento  | Optimizar librer√≠as de manipulaci√≥n de video (FFmpeg)          | Media     |
| Observabilidad | Monitorear m√©tricas del worker con Celery Flower o Prometheus  | Media     |

üìö **Autor(es):**  
Grupo 1  
Maestr√≠a en Ingenier√≠a de Software ‚Äì Universidad de los Andes
