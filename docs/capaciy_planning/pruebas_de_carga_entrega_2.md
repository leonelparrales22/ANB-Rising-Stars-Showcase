# ğŸ“‹ Resultados y AnÃ¡lisis de Capacidad â€“ Pruebas de Rendimiento

## ğŸ§ª Escenario 1 â€“ Prueba de Carga y EstrÃ©s sobre la API

### ğŸ” DescripciÃ³n General

Este escenario evalÃºa la capacidad de la API desarrollada en **FastAPI (Python)** frente a mÃºltiples usuarios concurrentes enviando solicitudes de subida y consulta de videos.  
El objetivo es determinar el punto mÃ¡ximo de carga antes de que los tiempos de respuesta o la tasa de error excedan los criterios de aceptaciÃ³n definidos.

---

### ğŸ“Š Resultados del Escenario

| MÃ©trica                                   | Valor Observado | Unidad       | Umbral Esperado | Cumple |
| ----------------------------------------- | --------------- | ------------ | --------------- | ------ |
| Usuarios concurrentes mÃ¡ximos sostenibles |                 | usuarios     | â‰¥ 200           |        |
| Latencia promedio (p50)                   |                 | ms           | â‰¤ 1000          |        |
| Latencia p95                              |                 | ms           | â‰¤ 1500          |        |
| Throughput promedio                       |                 | req/s        | â‰¥ 50            |        |
| % de errores (4xx / 5xx)                  |                 | %            | â‰¤ 5%            |        |
| Uso CPU promedio API                      |                 | %            | â‰¤ 90%           |        |
| Uso RAM promedio API                      |                 | %            | â‰¤ 80%           |        |
| Crecimiento de la cola RabbitMQ           |                 | mensajes/min | Controlado      |        |

> ğŸ§¾ **Nota:** completar con los valores obtenidos desde JMeter y mÃ©tricas del sistema (Prometheus / docker stats).

---

### ğŸ§  AnÃ¡lisis Detallado de Capacidad

- **Rendimiento general:**  
  _(Describe cÃ³mo se comportÃ³ la API bajo carga: estabilidad, latencias crecientes, comportamiento de la cola, saturaciÃ³n de CPU, etc.)_

- **IdentificaciÃ³n de cuellos de botella:**  
  _(Ejemplo: el tiempo de escritura en el almacenamiento fue el principal limitante o el nÃºmero de workers concurrentes en Celery no fue suficiente)._

- **LÃ­mite de capacidad identificado:**  
  _(Indica cuÃ¡ntos usuarios concurrentes puede manejar la API manteniendo los SLO definidos)._

---

### âœ… Conclusiones Derivadas

- _(Redacta 2 a 3 conclusiones claras sobre el comportamiento del sistema durante las pruebas de estrÃ©s: estabilidad, eficiencia, lÃ­mites de rendimiento, etc.)_

Ejemplo:

> - La API mantuvo una latencia promedio menor a 1 segundo hasta 180 usuarios concurrentes.
> - A partir de 220 usuarios, se observÃ³ un aumento progresivo de errores HTTP 500 debido a saturaciÃ³n del worker.
> - La infraestructura actual puede sostener una carga media sin degradaciÃ³n perceptible en los tiempos de respuesta.

---

### ğŸš€ Recomendaciones para Escalar la SoluciÃ³n

| Ãrea           | RecomendaciÃ³n                                                       | Prioridad |
| -------------- | ------------------------------------------------------------------- | --------- |
| API            | Implementar balanceo de carga (Nginx + mÃºltiples rÃ©plicas)          | Alta      |
| Worker         | Incrementar nÃºmero de instancias Celery o hilos por nodo            | Alta      |
| Base de datos  | Revisar Ã­ndices en tablas de videos y usuarios                      | Media     |
| Almacenamiento | Mover a servicio externo (S3 o similar) para descargas concurrentes | Media     |
| Observabilidad | Integrar mÃ©tricas automÃ¡ticas con Prometheus + Grafana              | Media     |
| Caching        | Aplicar cache Redis para consultas frecuentes                       | Baja      |

---

## ğŸ§ª Escenario 2 â€“ Prueba de Capacidad del Worker (Procesador de Videos)

### ğŸ” DescripciÃ³n General

Este escenario mide la capacidad del **worker (Celery)** para procesar videos de diferentes tamaÃ±os bajo distintos niveles de paralelismo, evaluando la eficiencia del procesamiento en segundo plano.

---

### ğŸ“Š Resultados del Escenario

| ConfiguraciÃ³n | TamaÃ±o Video | Paralelismo | Videos Procesados | Tiempo Promedio | Throughput | CPU Worker | RAM Worker |
| ------------- | ------------ | ----------- | ----------------- | --------------- | ---------- | ---------- | ---------- |
| Config 1      | 50 MB        | 1 hilo      |                   |                 |            |            |            |
| Config 2      | 50 MB        | 2 hilos     |                   |                 |            |            |            |
| Config 3      | 100 MB       | 1 hilo      |                   |                 |            |            |            |
| Config 4      | 100 MB       | 4 hilos     |                   |                 |            |            |            |

> ğŸ“˜ **Sugerencia:** anotar tiempos medidos con Celery logs o mÃ©tricas personalizadas.

---

### ğŸ§  AnÃ¡lisis Detallado de Capacidad

- **DesempeÃ±o por tamaÃ±o de video:**  
  _(Describe el comportamiento observado segÃºn el tamaÃ±o de los archivos y la cantidad de hilos)._

- **Uso de recursos:**  
  _(Analiza el consumo de CPU y memoria del worker, la velocidad de escritura en disco, y el impacto sobre RabbitMQ y PostgreSQL)._

- **LÃ­mite de capacidad:**  
  _(Indica el punto mÃ¡ximo de throughput alcanzado sin crecimiento sostenido de la cola)._

---

### âœ… Conclusiones Derivadas

- _(Resume los hallazgos mÃ¡s relevantes del worker: capacidad de procesamiento, estabilidad, limitantes de hardware, etc.)_

Ejemplo:

> - El worker logrÃ³ procesar 20 videos/minuto de 50MB con 2 hilos activos.
> - El rendimiento disminuyÃ³ con archivos de 100MB debido a limitaciones de I/O.
> - El uso de CPU se mantuvo por debajo del 80% durante toda la ejecuciÃ³n.

---

### ğŸš€ Recomendaciones para Escalar la SoluciÃ³n

| Ãrea           | RecomendaciÃ³n                                                  | Prioridad |
| -------------- | -------------------------------------------------------------- | --------- |
| Worker         | Aumentar el nÃºmero de instancias Celery en nodos separados     | Alta      |
| RabbitMQ       | Habilitar colas dedicadas por tipo de tarea                    | Media     |
| Storage        | Incorporar almacenamiento SSD o servicio externo con mayor I/O | Alta      |
| Procesamiento  | Optimizar librerÃ­as de manipulaciÃ³n de video (FFmpeg)          | Media     |
| Observabilidad | Monitorear mÃ©tricas del worker con Celery Flower o Prometheus  | Media     |

---

## ğŸ“„ ConclusiÃ³n General

_(Espacio para un resumen global de ambas pruebas, limitaciones del sistema y prÃ³ximos pasos para optimizaciÃ³n de rendimiento.)_

Ejemplo:

> Las pruebas evidencian que la arquitectura actual soporta adecuadamente la carga esperada del entorno universitario.  
> Sin embargo, se identifican oportunidades de mejora en el procesamiento paralelo y en la infraestructura de almacenamiento.  
> Se recomienda priorizar el escalamiento horizontal del worker y el uso de cachÃ© en la API para optimizar tiempos de respuesta.

---

ğŸ“š **Autor(es):**  
Grupo 1  
MaestrÃ­a en IngenierÃ­a de Software â€“ Universidad de los Andes
